{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08579f2-f6b7-46c0-a6e0-536e1ccee570",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae0e92e-20bd-4a7f-b631-8139f5b610e2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "050ff0c7-bc9d-4a9c-ab1e-96e24255cb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "637e3d2e-5330-4cdf-925f-138b13132025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "\n",
    "# 3rd party libraries\n",
    "import geopandas as gpd\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import sqlalchemy as sa\n",
    "#import pickle\n",
    "\n",
    "# Local libraries\n",
    "import pudl\n",
    "#from pudl.analysis.fill_ferc1_fuel_gaps import *\n",
    "#from pudl.analysis.flag_ferc1_totals import *\n",
    "\n",
    "# Enable viewing of logging outputs\n",
    "logger=logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler(stream=sys.stdout)\n",
    "formatter = logging.Formatter('%(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.handlers = [handler]\n",
    "\n",
    "# Display settings\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.dpi'] = 75\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a1ec87c-47f2-452e-a004-66e7ef949245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish connection to pudl database\n",
    "pudl_settings = pudl.workspace.setup.get_defaults()\n",
    "pudl_engine = sa.create_engine(pudl_settings['pudl_db'])\n",
    "pudl_out = pudl.output.pudltabl.PudlTabl(\n",
    "    pudl_engine=pudl_engine,\n",
    "    freq='AS'\n",
    ")\n",
    "\n",
    "ferc_engine = sa.create_engine(pudl_settings['ferc1_db'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9315e30b-e30a-4447-847d-eb3b73bff4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_plants_raw = pd.read_sql(\"f1_gnrt_plant\", ferc_engine)\n",
    "small_plants = pudl_out.plants_small_ferc1()#.dropna(subset=['plant_name_ferc1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f169e854-0e06-416e-ba20-0e66a1712c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aesharpe/miniconda3/envs/pudl-dev/lib/python3.9/site-packages/openpyxl/worksheet/_reader.py:312: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Here we create a fake raw dfs dictionary with just the small plants df to run it through\n",
    "# Zane's existing transform feature.\n",
    "fake_dict = {'plants_small_ferc1': small_plants_raw}\n",
    "new_dict = {}\n",
    "small_plants_dict = pudl.transform.ferc1.plants_small(fake_dict, new_dict)\n",
    "small_plants_out = small_plants_dict['plants_small_ferc1']\n",
    "\n",
    "# drop rows with no plant name because we can't use that\n",
    "small_plants_out = small_plants_out.dropna(subset=['plant_name_ferc1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5043f8b6-6d8d-45f7-ab21-4a59f5f82693",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9e871fa-6bdc-4023-8a79-9fba1bd5a94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If these columns are nan, we can assume it is either a header row or isn't useful\n",
    "nan_cols = ['construction_year', 'net_generation_mwh', 'total_cost_of_plant', 'capex_per_mw', 'opex_total', \n",
    "            'opex_fuel', 'opex_maintenance', 'fuel_cost_per_mmbtu']\n",
    "\n",
    "# If a potential header column has these strings, it's probably a useful header\n",
    "header_strings = ['hydro', 'hyrdo', 'internal', 'wind', 'solar', 'gas', 'diesel', 'diesal', \n",
    "                  'steam', 'other', 'combustion', 'combustine', 'fuel cell', 'hydraulic', \n",
    "                  'waste', 'landfill', 'photovoltaic', 'nuclear', 'oil', 'renewable', \n",
    "                  'facilities', 'combined cycle']\n",
    "\n",
    "# If a potential header has these strings, it is not a header...\n",
    "exclude = ['#', '\\*', 'pg', 'solargenix', 'solargennix', '\\@', 'rockton', 'albany steam']\n",
    "\n",
    "# ...unless it also has one of these strings\n",
    "exceptions = ['hydro plants: licensed proj. no.', 'hydro license no.', \n",
    "              'hydro: license no.', 'hydro plants: licensed proj no.']\n",
    "\n",
    "# What we will rename the headers once we remove them as rows \n",
    "new_header_labels = {\n",
    "    'hydroelectric': ['hydro', 'hyrdo'],\n",
    "    'internal combustion': ['internal', 'interal', 'international combustion'],\n",
    "    'combustion turbine': ['combustion turbine'],\n",
    "    'combined cycle': ['combined cycle'],\n",
    "    'gas turbine': ['gas'],\n",
    "    'petroleum liquids': ['oil', 'diesel', 'diesal'],\n",
    "    'solar': ['solar', 'photovoltaic'],\n",
    "    'wind': ['wind'],\n",
    "    'geothermal': ['geothermal'],\n",
    "    'waste': ['waste', 'landfill'],\n",
    "    'steam': ['steam'],\n",
    "    'nuclear': ['nuclear'],\n",
    "    'fuel_cell': ['fuel cell']\n",
    "}\n",
    "\n",
    "# Header names that match the one's that zane used in his manual mapping (so we can \n",
    "# compare processes)\n",
    "zane_header_labels = {\n",
    "    'solar_pv': ['solar', 'photovoltaic'],\n",
    "    'wind': ['wind'],\n",
    "    'hydro': ['hydro', 'hyrdo'],\n",
    "    'internal_combustion': ['internal', 'interal', 'international combustion', ],\n",
    "    'combustion_turbine': ['combustion turbine', 'combustine turbine'],\n",
    "    'combined_cycle': ['combined cycle'],\n",
    "    'diesel_turbine': ['oil', 'diesel', 'diesal'],\n",
    "    'gas_turbine': ['gas'],\n",
    "    'geothermal': ['geothermal'],\n",
    "    'waste_heat': ['waste', 'landfill'],\n",
    "    'steam_heat': ['steam'],\n",
    "    'nuclear': ['nuclear'],\n",
    "    'fuel_cell': ['fuel cell']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbadfb65-d686-4f93-9b84-f2389f9a4ca0",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4c6085b-c581-4d68-937e-e565092af7ce",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMOVED NAN VALUES: \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 233 entries, 0 to 19552\n",
      "Data columns (total 8 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   construction_year    0 non-null      float64\n",
      " 1   net_generation_mwh   0 non-null      float64\n",
      " 2   total_cost_of_plant  0 non-null      float64\n",
      " 3   capex_per_mw         0 non-null      float64\n",
      " 4   opex_total           0 non-null      float64\n",
      " 5   opex_fuel            0 non-null      float64\n",
      " 6   opex_maintenance     0 non-null      float64\n",
      " 7   fuel_cost_per_mmbtu  0 non-null      float64\n",
      "dtypes: float64(8)\n",
      "memory usage: 16.4 KB\n"
     ]
    }
   ],
   "source": [
    "# Remove utilities with all NAN rows because these won't contain anything meaningful\n",
    "# spc = small_plants_clean\n",
    "spc = (\n",
    "    small_plants_out\n",
    "    .groupby('utility_id_ferc1').filter(lambda x: ~x[nan_cols].isna().all().all())\n",
    ")\n",
    "# Show what was removed\n",
    "print('REMOVED NAN VALUES: \\n')\n",
    "pd.concat([small_plants_out,spc]).drop_duplicates(keep=False)[nan_cols].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f44f7039-86be-47fb-9586-4bf762906c4c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMOVED NAN NAMES:\n",
      "                                        183\n",
      "none                                    81\n",
      "------------------                      25\n",
      "not applicable                          22\n",
      "-------------------                     16\n",
      "na                                       8\n",
      "n/a                                      7\n",
      "-----                                    3\n",
      "-----------                              3\n",
      "--------------------                     2\n",
      "------------------------                 1\n",
      "-------------------------                1\n",
      "------------                             1\n",
      "----------------                         1\n",
      "-------------                            1\n",
      "---------------------------------        1\n",
      "-----------------------------------      1\n",
      "Name: plant_name_ferc1, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with ----------or '' for names\n",
    "spc2 = spc[~spc['plant_name_ferc1'].str.contains('---')].copy()\n",
    "spc3 = spc2[~spc2['plant_name_ferc1'].isin(['', 'none', 'na', 'n/a', 'not applicable'])].reset_index(drop=True)\n",
    "\n",
    "# Show what was removed\n",
    "print('REMOVED NAN NAMES:\\n', pd.concat([spc, spc3]).drop_duplicates(keep=False).plant_name_ferc1.value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d49246b-2d7b-4caa-a0cf-b53f35e08b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL ROWS REMOVED: 590 rows. Current row total: 18969\n"
     ]
    }
   ],
   "source": [
    "# Show total rows removed\n",
    "print(f'TOTAL ROWS REMOVED: {len(small_plants_out) - len(spc3)} rows. Current row total: {len(spc3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b140ae5-1232-4e57-bf68-9f133ff0bd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find NOTE CLUMPS\n",
    "\n",
    "# Add some new columns\n",
    "spc3.insert(3, 'is_header', False)\n",
    "spc3.insert(3, 'header_type', np.nan)\n",
    "\n",
    "# Label possible header rows (based on the nan cols specified above)\n",
    "spc3.loc[spc3.filter(nan_cols).isna().all(1), 'is_header'] = True\n",
    "\n",
    "# Label good header rows (based on whether they contain key strings)\n",
    "is_header = spc3['is_header']\n",
    "is_good_header = spc3['plant_name_ferc1'].str.contains('|'.join(header_strings))\n",
    "not_bad = ~spc3['plant_name_ferc1'].str.contains('|'.join(exclude))\n",
    "\n",
    "spc3.loc[is_header & is_good_header & not_bad, 'header_type'] = 'good_header'\n",
    "spc3.loc[spc3['plant_name_ferc1'].isin(exceptions), 'header_type'] = 'good_header'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2f41d5-426b-46a3-bcb1-55756ab10ad3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afa09ef-90d4-45fa-9ab2-b36cef99e1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pudl.analysis.fill_ferc1_fuel_gaps import create_groups\n",
    "\n",
    "def get_header_clumps_all(df):\n",
    "    \"\"\"\n",
    "    Remove clumps of consecutive rows flagged as possible headers.\n",
    "    \n",
    "    FERC has lots of note rows that are not headers but are also not useful for analysis.\n",
    "    This function looks for rows flagged as possible headers (based on NAN values) and checks to\n",
    "    see if there are multiple in a row. A header row is (usually) defined as a row with NAN values\n",
    "    followed by rows without NAN values, so when there are more than one clumped together they are\n",
    "    likely either notes or not helpful.\n",
    "    \n",
    "    Sometimes note clumps will end with a meaningful header. This function also checks for this and will\n",
    "    unclump any headers at the bottom of clumps. There is one exception to this case which is a header that \n",
    "    is followed by a plant that had no values reported... Unfortunately I haven't built a work around,\n",
    "    but hopefully there aren't very many of these. Currently, that header and plant will be categorized\n",
    "    as clumps and removed.\n",
    "    \n",
    "    \"\"\"\n",
    "    util_groups = df.groupby(['utility_id_ferc1', 'report_year'])\n",
    "    \n",
    "    def get_header_clumps(util_year_group):\n",
    "        \n",
    "        # Create mini groups that count pockets of true and false for each utility and year\n",
    "        # create_groups() is a function from the fill_ferc1_fuel_gaps module-- basically what\n",
    "        # it does is create a df where each row represents a clump of adjecent, equal values for\n",
    "        # a given column. Ex: a column of True, True, True, False, True, False, False, will\n",
    "        # appear as True, False, True, False with value counts for each\n",
    "        group, header_count = create_groups(util_year_group, 'is_header')\n",
    "        \n",
    "        # These are used later to enable exceptions\n",
    "        max_idx_val = header_count.index.max()\n",
    "        max_df_val = util_year_group.index.max()\n",
    "        \n",
    "        # Create a list of the index values that comprise each of the header clumps\n",
    "        # It's only considered a clump if it is greater than 1.\n",
    "        idx_list = list(header_count[\n",
    "            (header_count['fuel']) & (header_count['val_count'] > 1)].index)\n",
    "        \n",
    "        # If the last row is not a clump (i.e. there is just one value) but it is a header (i.e. has nan values)\n",
    "        # then also include it in the index values to be flagged because it might be a one-liner note. And\n",
    "        # because it is at the bottom there is no chance it can actually be a useful header because there are\n",
    "        # no value rows below it.\n",
    "        last_row = header_count.tail(1)\n",
    "        if (last_row['fuel'].item()) & (last_row['val_count'].item()==1):\n",
    "            idx_list = idx_list + list(last_row.index)\n",
    "        # If there are any clumped/end headers:\n",
    "        if idx_list:\n",
    "            for idx in idx_list:\n",
    "                # Check to see if last clump bit is not a header... sometimes you might find a clump of\n",
    "                # notes FOLLOWED by a useful header. This next bit will check the last row in each of\n",
    "                # the identified clumps and \"unclump\" it if it looks like a valid header. We only need\n",
    "                # to check clumps that fall in the middle because, as previously mentioned, the last row\n",
    "                # cannot contain any meaningful header information because there are no values below it.\n",
    "                idx_range = group.groups[idx+1]\n",
    "                is_middle_clump = group.groups[idx+1].max() < max_df_val\n",
    "                is_good_header = util_year_group.loc[\n",
    "                    util_year_group.index.isin(group.groups[idx+1])].tail(1)['plant_name_original'].str.contains('|'.join(header_strings)).all()  #.isin(header_strings).all()\n",
    "                # If the clump is in the middle and the last row looks like a header, then drop it from the idx range\n",
    "                if is_middle_clump & is_good_header:\n",
    "                    idx_range = [x for x in idx_range if x != idx_range.max()]\n",
    "                # Label the clump as a clump\n",
    "                util_year_group.loc[\n",
    "                    util_year_group.index.isin(idx_range), 'header_type'] = 'clump'\n",
    "        return util_year_group\n",
    "    \n",
    "    return util_groups.apply(lambda x: get_header_clumps(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
